{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d055fd15",
   "metadata": {},
   "source": [
    "# Corrective RAG\n",
    "- Focus is on improving the accuracy and relevance of generated docs by incorporating mechanisms for self reflection and self-grading of retrieved documents\n",
    "- It evaluates the quality of generated docs and applies corrective measures (e.g rewriting queries or augmenting with web search) when needed\n",
    "\n",
    "## Limitations of traditional RAGs\n",
    "- Traditional RAG systems heavily rely on accuracy of retrieved documents. If the retrieved information is flawed or incomplete, the generated response can also be inaccurate\n",
    "\n",
    "## Corrective RAG's core components:\n",
    "- **Retrieval Evaluator**: Assesses the quality and relevance of retrieved documents\n",
    "- **Generative Model**: Generates an initial response based on retrieved information\n",
    "- **Refinement and Correction**: CRAG employs strategies like Knowledge Refinement or web search to addres issues identified by the retrieval evaluator\n",
    "\n",
    "## Benefits of CRAG:\n",
    "- Improved Accuracy\n",
    "- Enhanced Relevance\n",
    "- Increased Robustness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea894d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_groq import ChatGroq\n",
    "load_dotenv()\n",
    "os.environ['GROQ_API_KEY'] = os.getenv('GROQ_API_KEY')\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')\n",
    "os.environ['TAVILY_API_KEY'] = os.getenv('TAVILY_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0406a12c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "## Build an index\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "\n",
    "# embeddings\n",
    "embd = OpenAIEmbeddings()\n",
    "\n",
    "# Docs to index\n",
    "urls = [\n",
    "    'https://lilianweng.github.io/posts/2023-06-23-agent/',\n",
    "    'https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/',\n",
    "    'https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/',\n",
    "]\n",
    "\n",
    "# load\n",
    "docs = [WebBaseLoader(url).load() for url in urls]\n",
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "\n",
    "# split text\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=500, chunk_overlap=0)\n",
    "doc_splits = text_splitter.split_documents(docs_list)\n",
    "\n",
    "## Vector stores\n",
    "vector_store = FAISS.from_documents(\n",
    "    documents = doc_splits,\n",
    "    embedding=OpenAIEmbeddings()\n",
    ")\n",
    "\n",
    "## vector store as retriever\n",
    "retriever = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e14687fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary_score='yes'\n"
     ]
    }
   ],
   "source": [
    "## Retrieval grader\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Data Model\n",
    "class GradeDocuments(BaseModel):\n",
    "    '''Binary Score for relevance check on retrieved documents'''\n",
    "    \n",
    "    binary_score:str = Field(description='Documents are relevant to question \\'yes\\' or \\'no\\'')\n",
    "    \n",
    "## LLM with function call\n",
    "llm = ChatOpenAI(model='gpt-4o', temperature=0)\n",
    "structured_llm_grader = llm.with_structured_output(GradeDocuments)\n",
    "\n",
    "# prompt\n",
    "system = \"\"\" You're a grader assessing relevance of the retrieved document to a user question. \\n\n",
    "If the document contains keyword(s) or semantic meaning related to the question, grade it as relevant. \\n\n",
    "Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\n",
    "\"\"\"\n",
    "grade_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        ('system', system),\n",
    "        ('human', 'Retrieved document: \\n\\n {document} \\n\\n User questions: {question}')\n",
    "    ]\n",
    ")\n",
    "\n",
    "retrieval_grader = grade_prompt | structured_llm_grader\n",
    "question = 'agent memory'\n",
    "docs = retriever.invoke(question)\n",
    "doc_txt = docs[1].page_content\n",
    "print(retrieval_grader.invoke({'question':question, 'document':doc_txt}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3eb77718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In LLM-powered autonomous agents, memory is divided into short-term and long-term components. Short-term memory involves in-context learning, while long-term memory allows the agent to retain and recall information over extended periods, often using an external vector store for fast retrieval. This memory system enables agents to learn from past experiences and improve future actions.\n"
     ]
    }
   ],
   "source": [
    "## generate\n",
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# prompt\n",
    "prompt = hub.pull('rlm/rag-prompt')\n",
    "\n",
    "#LLM\n",
    "llm = ChatOpenAI(model='gpt-4o', temperature=0)\n",
    "\n",
    "# post processing\n",
    "def format_docs(docs):\n",
    "    return '\\n\\n'.join(doc.page_content for doc in docs)\n",
    "\n",
    "# chain\n",
    "rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# Run\n",
    "generation = rag_chain.invoke({'context': docs, 'question':question})\n",
    "print(generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f4dee41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is agent memory and how does it function in artificial intelligence systems?'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Question rewriter\n",
    "\n",
    "#LLM\n",
    "llm = ChatOpenAI(model='gpt-4o', temperature=0)\n",
    "\n",
    "# prompt\n",
    "system = ''' You are a question re-writer that converts an input question to a better version that is optimized \\n\n",
    "for web search. Look at my input and try to reason about the underlying semantic intent/ meaning\n",
    "'''\n",
    "\n",
    "rewrite_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        ('system', system),\n",
    "        ('human', 'Here is the initial question: \\n\\n {question} \\n Generate an improved question')\n",
    "    ]\n",
    ")\n",
    "\n",
    "question_rewriter = rewrite_prompt | llm | StrOutputParser()\n",
    "question_rewriter.invoke({'question':question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9bf4a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/15/xbwr_x_d51sf0lj4mptglw6h0000gn/T/ipykernel_50901/484733239.py:3: LangChainDeprecationWarning: The class `TavilySearchResults` was deprecated in LangChain 0.3.25 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-tavily package and should be used instead. To use it run `pip install -U :class:`~langchain-tavily` and import as `from :class:`~langchain_tavily import TavilySearch``.\n",
      "  websearchtool = TavilySearchResults(k=3)\n"
     ]
    }
   ],
   "source": [
    "# Search\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "websearchtool = TavilySearchResults(k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "adaf8344",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph\n",
    "\n",
    "    Attributes:\n",
    "       question: question\n",
    "       generation: LLM generation\n",
    "       web_search: whether to add web search\n",
    "       documents: list of documents\n",
    "    \"\"\"\n",
    "    \n",
    "    question:str\n",
    "    generation:str\n",
    "    web_search:str\n",
    "    documents: List[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "940f42ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "def retrieve(state):\n",
    "    \"\"\"Retrieve documents \n",
    "\n",
    "    Args:\n",
    "        state (dict): The current state of graph\n",
    "        \n",
    "    Returns:\n",
    "        state (dict): New key added to the state, documents that contain retrieved documents\n",
    "    \"\"\"\n",
    "    print('--- RETRIEVE ---')\n",
    "    question = state['question']\n",
    "    # Retrieval\n",
    "    documents = retriever.invoke(question)\n",
    "    return {'documents': documents, 'question':question}    \n",
    "\n",
    "def generate(state):\n",
    "    \"\"\"Generate answer\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current state of the graph\n",
    "    \n",
    "    Return: \n",
    "        state (dict):  New key added to the state, generation, that contains LLM generation\n",
    "    \"\"\"\n",
    "    print('--- GENERATE ---')\n",
    "    question = state['question']\n",
    "    documents = state['documents']\n",
    "    \n",
    "    ## RAG generation\n",
    "    generation = rag_chain.invoke({'context': documents,'question':question})\n",
    "    return {'documents': documents,'question':question, 'generation':generation}\n",
    "\n",
    "def grade_documents(state):\n",
    "    \"\"\"Determines whether the retrieved documents are relevant to the question\n",
    "\n",
    "    Args:\n",
    "        state (dict): Current state of the graph\n",
    "        \n",
    "    Returns:\n",
    "        state (dict): Updates the documents key with only the filtered relevant documents\n",
    "    \"\"\"\n",
    "    print('--- CHECK DOCUMENT RELEVANCE TO THE QUESTION ---')\n",
    "    question = state['question']\n",
    "    documents = state['documents']\n",
    "    \n",
    "    # Score each doc\n",
    "    filtered_docs = []\n",
    "    web_search = 'No'\n",
    "    for d in documents:\n",
    "        score= retrieval_grader.invoke({\n",
    "            'question': question, 'document':d.page_content\n",
    "        })\n",
    "        \n",
    "        grade = score.binary_score\n",
    "        if grade == 'yes':\n",
    "            print('--- GRADE: DOCUMENT RELEVANT ---')\n",
    "            filtered_docs.append(d)\n",
    "        else:\n",
    "            print('--- GRADE: DOCUMENT NOT RELEVANT ---')\n",
    "            web_search = 'yes'\n",
    "            continue\n",
    "    return {'documents':documents, 'question': question, 'web_search': web_search}\n",
    "\n",
    "def transform_query(state):\n",
    "    \"\"\"Transform the query to produce a better question\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current state of the graph\n",
    "        \n",
    "    Returns:\n",
    "        state (dict): Updates the key with re-phrased question\n",
    "    \"\"\"\n",
    "    \n",
    "    print('--- TRANSFORM QUERY ---')\n",
    "    question = state['question']\n",
    "    documents = state['documents']\n",
    "    \n",
    "    # rewrite question\n",
    "    better_question = question_rewriter.invoke({'question':question})\n",
    "    return {'documents':documents, 'question':question}\n",
    "\n",
    "def web_search(state):\n",
    "    \"\"\"Web search based on rephrased question\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "        \n",
    "    Returns:\n",
    "        state (dict): Updates documents key with appended web results\n",
    "    \"\"\"\n",
    "    \n",
    "    print('--- WEB SEARCH ---')\n",
    "    question = state['question']\n",
    "    documents = state['documents']\n",
    "    \n",
    "    # web search\n",
    "    docs = websearchtool.invoke({'query':question})\n",
    "    web_results = '\\n'.join([d['content'] for d in docs])\n",
    "    web_results= Document(page_content=web_results)\n",
    "    documents.append(web_results)\n",
    "    return {'documents':documents, 'question':question}\n",
    "\n",
    "def decide_to_generate(state):\n",
    "    \"\"\"Determines whether to generate an answer or re-generate the question\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "        \n",
    "    Returns:\n",
    "        str: Binary decision for next node call\n",
    "    \"\"\"\n",
    "    \n",
    "    print('--- ASSESS GRADED DOCUMENTS ---')\n",
    "    web_search = state['web_search']\n",
    "    if web_search == 'yes':\n",
    "        print(\" --- DECISION: DOCS NOT RELEVANT TO QUERY, TRANSFORM QUERY ---\")\n",
    "        return 'transform_query'\n",
    "    else:\n",
    "        print('--- DECISION: DOCS RELEVANT TO QUERY, GENERATE ANSWER ---')\n",
    "        return 'generate'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a9bcf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## build the graph\n",
    "from langgraph.graph import START,StateGraph, END\n",
    "from IPython.display import Image, display\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "## add nodes\n",
    "workflow.add_node('retrieve', retrieve)\n",
    "workflow.add_node('grade_documents', grade_documents)\n",
    "workflow.add_node('generate', generate)\n",
    "workflow.add_node('transform_query', transform_query)\n",
    "workflow.add_node('web_search', web_search)\n",
    "\n",
    "# build graph\n",
    "workflow.add_edge(START, 'retrieve')\n",
    "workflow.add_edge('retrieve', 'grade_documents')\n",
    "workflow.add_conditional_edges(\n",
    "    'grade_documents',\n",
    "    decide_to_generate,\n",
    "    {\n",
    "        'transform_query': 'transform_query',\n",
    "        'generate': 'generate'\n",
    "    }\n",
    ")\n",
    "workflow.add_edge('transform_query', 'web_search')\n",
    "workflow.add_edge('web_search', 'generate')\n",
    "workflow.add_edge('generate', END)\n",
    "\n",
    "graph = workflow.compile()\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langgraph-agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
